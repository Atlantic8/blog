---
title: Entropy and Related Metrics
mathjax: true
date: 2020-09-01 23:01:41
tags: 
categories: Math
---

##### 熵

首先说**香农信息量**（也叫自信息self-information），其定义为
$$
\log\frac{1}{p}=-\log p
$$
**熵的本质是香农信息量的期望**，即
$$
H(p)=-\sum_ip(i)* \log p(i)
$$
表示混乱程度，熵越大越混乱。

---

##### 交叉熵

现有关于样本集的2个概率分布$p$和$q$，其中$p$为真实分布，$q$非真实分布。用**分布$q$来表示来自分布$p$的平均编码长度的期望**(即平均编码长度)为：
$$
H(p,q)=-\sum_{i}^{} p(i)*\log q(i) 
$$
其中，$H(p,q)$就叫做**交叉熵**。

举个例子，有集合$S=(A,B,C,D)$，真实分布$p=(1/2,1/2,0,0)$，即$A,B$出现的概率都是1/2。现在用分布$q$来编码分布$p$，则有：
- 假设$q=(1/2,1/2,0,0)$，即真实分布，根据上式计算$H(p,q)=1$，即只需要1位编码就可以识别$A,B$
- 假设$q=(1/4,1/4,1/4,1/4)$，计算得$H(p,q)=2$，则需要2位编码就可以识别集合$S=(A,B,C,D)$

非真实分布编码所需长度**大于**真实分布编码所需长度，即存在
$$
H(p,q) \ge H(p,p)=H(p)
$$
当且仅当$p=q$时等号成立。所以交叉熵作为损失函数时，可以表示两个分布之间的相似性，越相似交叉熵越小。

---

##### KL散度（相对熵）
定义：设真实分布为$p$，**由分布$q$得到的平均编码长度比由$p$得到的平均编码长度多出的bit数**称为“相对熵”，写作
$$
D(p||q) = H(p,q)-H(p)=\sum_{i}^{} p(i)*\log \frac{p(i)}{q(i)}
$$
相对熵也叫KL散度（Kullback–Leibler divergence），**表示2个函数或概率分布的差异性**，差异越大则相对熵越大，相同时KL散度为0。注意，KL散度是**非对称**的。

TD-IDF算法就可以理解为相对熵的应用：词频在整个语料库的分布与词频在具体文档中分布（$\log \frac{|N|}{|N_t|}$）之间的差异性


---

##### JS散度
KL散度的一种变形，定义如下：
$$
JS(P||Q)=\frac{1}{2}KL\left( P(x)||\frac{P(x)+Q(x)}{2} \right) + \frac{1}{2}KL\left( P(x)||\frac{Q(x)+Q(x)}{2} \right)
$$
JS散度有如下特性：
- 对称性
- 值域范围是[0,1]，相同则是0，相反为1



